{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "pdf = open('sample.pdf', 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdf)\n",
    "print(pdfReader.numPages)\n",
    "pageObj = pdfReader.getPage(1)\n",
    "print(pageObj.extractText()) \n",
    "pdf.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "text = textract.process(\"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open(r'sample.pdf') as pdf:\n",
    "    first_page = pdf.pages[0]\n",
    "    text = first_page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google Research Football: A Novel Reinforcement Learning Environment\\nKarolKurach∗ AntonRaichuk(cid:63) PiotrStan´czyk(cid:63) MichałZaja¸c†\\nOlivierBachem LasseEspeholt CarlosRiquelme DamienVincent\\nMarcinMichalski OlivierBousquet SylvainGelly\\nGoogleResearch,BrainTeam\\n0\\n2\\n0\\nAbstract\\n2\\n  Recent progress in the ﬁeld of reinforcement learning has\\nr\\np been accelerated by virtual learning environments such\\nA as video games, where novel algorithms and ideas can\\nbe quickly tested in a safe and reproducible manner. We\\n \\n4 introduce the Google Research Football Environment, a\\n1 new reinforcement learning environment where agents are\\n  trained to play football in an advanced, physics-based 3D\\n \\n] simulator. The resulting environment is challenging, easy\\nG\\ntouseandcustomize,anditisavailableunderapermissive\\nL open-source license. In addition, it provides support for\\n. multiplayer and multi-agent experiments. We propose three\\ns full-game scenarios of varying difﬁculty with the Football\\nc\\nBenchmarksandreportbaselineresultsforthreecommonly\\n[ Figure 1: The Google Research Football Environment\\n  used reinforcement algorithms (IMPALA, PPO, and Ape-X\\n  (github.com/google-research/football) pro-\\n2 DQN). We also provide a diverse set of simpler scenarios\\nv withtheFootballAcademyandshowcaseseveralpromising vides a novel reinforcement learning environment where\\n0 researchdirections. agents are trained to play football in an advance, physics\\n8 based3Dsimulation.\\n1 Introduction\\n1\\n1 The goal of reinforcement learning (RL) is to train smart\\n. agents that can interact with their environment and solve which we discuss in detail in the next section. For exam-\\n7\\ncomplex tasks (Sutton and Barto, 2018). Real-world appli- ple, they may either be too easy to solve for state-of-the-\\n0\\ncationsincluderobotics(Haarnojaetal.,2018),self-driving art algorithms or require access to large amounts of com-\\n9\\ncars (Bansal, Krizhevsky, and Ogale, 2018), and control putational resources. At the same time, they may either be\\n1\\n: problems such as increasing the power efﬁciency of data (near-)deterministic or there may even be a known model\\nv centers (Lazic et al., 2018). Yet, the rapid progress in this of the environment (such as in Go or Chess). Similarly,\\nXi ﬁeld has been fueled by making agents play games such manylearningenvironmentsareinherentlysingleplayerby\\nas the iconic Atari console games (Bellemare et al., 2013; only modeling the interaction of an agent with a ﬁxed en-\\nr\\na Mnih et al., 2013), the ancient game of Go (Silver et al., vironmentortheyfocusonasingleaspectofreinforcement\\n2016), or professionally played video games like Dota 2 learningsuchascontinuouscontrolorsafety.Finally,learn-\\n(OpenAI, 2019) or Starcraft II (Vinyals et al., 2017). The ingenvironmentsmayhaverestrictivelicensesordependon\\nreason for this is simple: games provide challenging envi- closedsourcebinaries.\\nronments where new algorithms and ideas can be quickly ThishighlightstheneedforaRLenvironmentthatisnot\\ntestedinasafeandreproduciblemanner. only challenging from a learning standpoint and customiz-\\nWhile a variety of reinforcement learning environments able in terms of difﬁculty but also accessible for research\\nexist, they often come with a few drawbacks for research, bothintermsoflicensingandintermsofrequiredcomputa-\\ntionalresources.Moreover,suchanenvironmentshouldide-\\n∗Indicates equal authorship. Correspondence to Karol Kurach\\nally provide the tools to a variety of current reinforcement\\n(kkurach@google.com).\\nlearningresearchtopicssuchastheimpactofstochasticity,\\n†Student at Jagiellonian University, work done during intern-\\nself-play,multi-agentsetupsandmodel-basedreinforcement\\nshipatGoogleBrain.\\nCopyright(cid:13)c 2020,AssociationfortheAdvancementofArtiﬁcial learning, while also requiring smart decisions, tactics, and\\nIntelligence(www.aaai.org).Allrightsreserved. strategiesatmultiplelevelsofabstraction.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"text.txt\",\"wt\", encoding=\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.writelines(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
