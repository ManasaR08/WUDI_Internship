{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"text.txt\", \"r\",encoding=\"utf-8\")\n",
    "text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google Research Football: A Novel Reinforcement Learning Environment\\nKarolKurach∗ AntonRaichuk(cid:63) PiotrStan´czyk(cid:63) MichałZaja¸c†\\nOlivierBachem LasseEspeholt CarlosRiquelme DamienVincent\\nMarcinMichalski OlivierBousquet SylvainGelly\\nGoogleResearch,BrainTeam\\n0\\n2\\n0\\nAbstract\\n2\\n  Recent progress in the ﬁeld of reinforcement learning has\\nr\\np been accelerated by virtual learning environments such\\nA as video games, where novel algorithms and ideas can\\nbe quickly tested in a safe and reproducible manner. We\\n \\n4 introduce the Google Research Football Environment, a\\n1 new reinforcement learning environment where agents are\\n  trained to play football in an advanced, physics-based 3D\\n \\n] simulator. The resulting environment is challenging, easy\\nG\\ntouseandcustomize,anditisavailableunderapermissive\\nL open-source license. In addition, it provides support for\\n. multiplayer and multi-agent experiments. We propose three\\ns full-game scenarios of varying difﬁculty with the Football\\nc\\nBenchmarksandreportbaselineresultsforthreecommonly\\n[ Figure 1: The Google Research Football Environment\\n  used reinforcement algorithms (IMPALA, PPO, and Ape-X\\n  (github.com/google-research/football) pro-\\n2 DQN). We also provide a diverse set of simpler scenarios\\nv withtheFootballAcademyandshowcaseseveralpromising vides a novel reinforcement learning environment where\\n0 researchdirections. agents are trained to play football in an advance, physics\\n8 based3Dsimulation.\\n1 Introduction\\n1\\n1 The goal of reinforcement learning (RL) is to train smart\\n. agents that can interact with their environment and solve which we discuss in detail in the next section. For exam-\\n7\\ncomplex tasks (Sutton and Barto, 2018). Real-world appli- ple, they may either be too easy to solve for state-of-the-\\n0\\ncationsincluderobotics(Haarnojaetal.,2018),self-driving art algorithms or require access to large amounts of com-\\n9\\ncars (Bansal, Krizhevsky, and Ogale, 2018), and control putational resources. At the same time, they may either be\\n1\\n: problems such as increasing the power efﬁciency of data (near-)deterministic or there may even be a known model\\nv centers (Lazic et al., 2018). Yet, the rapid progress in this of the environment (such as in Go or Chess). Similarly,\\nXi ﬁeld has been fueled by making agents play games such manylearningenvironmentsareinherentlysingleplayerby\\nas the iconic Atari console games (Bellemare et al., 2013; only modeling the interaction of an agent with a ﬁxed en-\\nr\\na Mnih et al., 2013), the ancient game of Go (Silver et al., vironmentortheyfocusonasingleaspectofreinforcement\\n2016), or professionally played video games like Dota 2 learningsuchascontinuouscontrolorsafety.Finally,learn-\\n(OpenAI, 2019) or Starcraft II (Vinyals et al., 2017). The ingenvironmentsmayhaverestrictivelicensesordependon\\nreason for this is simple: games provide challenging envi- closedsourcebinaries.\\nronments where new algorithms and ideas can be quickly ThishighlightstheneedforaRLenvironmentthatisnot\\ntestedinasafeandreproduciblemanner. only challenging from a learning standpoint and customiz-\\nWhile a variety of reinforcement learning environments able in terms of difﬁculty but also accessible for research\\nexist, they often come with a few drawbacks for research, bothintermsoflicensingandintermsofrequiredcomputa-\\ntionalresources.Moreover,suchanenvironmentshouldide-\\n∗Indicates equal authorship. Correspondence to Karol Kurach\\nally provide the tools to a variety of current reinforcement\\n(kkurach@google.com).\\nlearningresearchtopicssuchastheimpactofstochasticity,\\n†Student at Jagiellonian University, work done during intern-\\nself-play,multi-agentsetupsandmodel-basedreinforcement\\nshipatGoogleBrain.\\nCopyright(cid:13)c 2020,AssociationfortheAdvancementofArtiﬁcial learning, while also requiring smart decisions, tactics, and\\nIntelligence(www.aaai.org).Allrightsreserved. strategiesatmultiplelevelsofabstraction.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = decontracted(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google Research Football: A Novel Reinforcement Learning Environment\\nKarolKurach∗ AntonRaichuk(cid:63) PiotrStan´czyk(cid:63) MichałZaja¸c†\\nOlivierBachem LasseEspeholt CarlosRiquelme DamienVincent\\nMarcinMichalski OlivierBousquet SylvainGelly\\nGoogleResearch,BrainTeam\\n0\\n2\\n0\\nAbstract\\n2\\n  Recent progress in the ﬁeld of reinforcement learning has\\nr\\np been accelerated by virtual learning environments such\\nA as video games, where novel algorithms and ideas can\\nbe quickly tested in a safe and reproducible manner. We\\n \\n4 introduce the Google Research Football Environment, a\\n1 new reinforcement learning environment where agents are\\n  trained to play football in an advanced, physics-based 3D\\n \\n] simulator. The resulting environment is challenging, easy\\nG\\ntouseandcustomize,anditisavailableunderapermissive\\nL open-source license. In addition, it provides support for\\n. multiplayer and multi-agent experiments. We propose three\\ns full-game scenarios of varying difﬁculty with the Football\\nc\\nBenchmarksandreportbaselineresultsforthreecommonly\\n[ Figure 1: The Google Research Football Environment\\n  used reinforcement algorithms (IMPALA, PPO, and Ape-X\\n  (github.com/google-research/football) pro-\\n2 DQN). We also provide a diverse set of simpler scenarios\\nv withtheFootballAcademyandshowcaseseveralpromising vides a novel reinforcement learning environment where\\n0 researchdirections. agents are trained to play football in an advance, physics\\n8 based3Dsimulation.\\n1 Introduction\\n1\\n1 The goal of reinforcement learning (RL) is to train smart\\n. agents that can interact with their environment and solve which we discuss in detail in the next section. For exam-\\n7\\ncomplex tasks (Sutton and Barto, 2018). Real-world appli- ple, they may either be too easy to solve for state-of-the-\\n0\\ncationsincluderobotics(Haarnojaetal.,2018),self-driving art algorithms or require access to large amounts of com-\\n9\\ncars (Bansal, Krizhevsky, and Ogale, 2018), and control putational resources. At the same time, they may either be\\n1\\n: problems such as increasing the power efﬁciency of data (near-)deterministic or there may even be a known model\\nv centers (Lazic et al., 2018). Yet, the rapid progress in this of the environment (such as in Go or Chess). Similarly,\\nXi ﬁeld has been fueled by making agents play games such manylearningenvironmentsareinherentlysingleplayerby\\nas the iconic Atari console games (Bellemare et al., 2013; only modeling the interaction of an agent with a ﬁxed en-\\nr\\na Mnih et al., 2013), the ancient game of Go (Silver et al., vironmentortheyfocusonasingleaspectofreinforcement\\n2016), or professionally played video games like Dota 2 learningsuchascontinuouscontrolorsafety.Finally,learn-\\n(OpenAI, 2019) or Starcraft II (Vinyals et al., 2017). The ingenvironmentsmayhaverestrictivelicensesordependon\\nreason for this is simple: games provide challenging envi- closedsourcebinaries.\\nronments where new algorithms and ideas can be quickly ThishighlightstheneedforaRLenvironmentthatisnot\\ntestedinasafeandreproduciblemanner. only challenging from a learning standpoint and customiz-\\nWhile a variety of reinforcement learning environments able in terms of difﬁculty but also accessible for research\\nexist, they often come with a few drawbacks for research, bothintermsoflicensingandintermsofrequiredcomputa-\\ntionalresources.Moreover,suchanenvironmentshouldide-\\n∗Indicates equal authorship. Correspondence to Karol Kurach\\nally provide the tools to a variety of current reinforcement\\n(kkurach@google.com).\\nlearningresearchtopicssuchastheimpactofstochasticity,\\n†Student at Jagiellonian University, work done during intern-\\nself-play,multi-agentsetupsandmodel-basedreinforcement\\nshipatGoogleBrain.\\nCopyright(cid:13)c 2020,AssociationfortheAdvancementofArtiﬁcial learning, while also requiring smart decisions, tactics, and\\nIntelligence(www.aaai.org).Allrightsreserved. strategiesatmultiplelevelsofabstraction.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('\\\\r', ' ')\n",
    "text = text.replace('\\\\\"', ' ')\n",
    "text = text.replace('\\\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google Research Football: A Novel Reinforcement Learning Environment\\nKarolKurach∗ AntonRaichuk(cid:63) PiotrStan´czyk(cid:63) MichałZaja¸c†\\nOlivierBachem LasseEspeholt CarlosRiquelme DamienVincent\\nMarcinMichalski OlivierBousquet SylvainGelly\\nGoogleResearch,BrainTeam\\n0\\n2\\n0\\nAbstract\\n2\\n  Recent progress in the ﬁeld of reinforcement learning has\\nr\\np been accelerated by virtual learning environments such\\nA as video games, where novel algorithms and ideas can\\nbe quickly tested in a safe and reproducible manner. We\\n \\n4 introduce the Google Research Football Environment, a\\n1 new reinforcement learning environment where agents are\\n  trained to play football in an advanced, physics-based 3D\\n \\n] simulator. The resulting environment is challenging, easy\\nG\\ntouseandcustomize,anditisavailableunderapermissive\\nL open-source license. In addition, it provides support for\\n. multiplayer and multi-agent experiments. We propose three\\ns full-game scenarios of varying difﬁculty with the Football\\nc\\nBenchmarksandreportbaselineresultsforthreecommonly\\n[ Figure 1: The Google Research Football Environment\\n  used reinforcement algorithms (IMPALA, PPO, and Ape-X\\n  (github.com/google-research/football) pro-\\n2 DQN). We also provide a diverse set of simpler scenarios\\nv withtheFootballAcademyandshowcaseseveralpromising vides a novel reinforcement learning environment where\\n0 researchdirections. agents are trained to play football in an advance, physics\\n8 based3Dsimulation.\\n1 Introduction\\n1\\n1 The goal of reinforcement learning (RL) is to train smart\\n. agents that can interact with their environment and solve which we discuss in detail in the next section. For exam-\\n7\\ncomplex tasks (Sutton and Barto, 2018). Real-world appli- ple, they may either be too easy to solve for state-of-the-\\n0\\ncationsincluderobotics(Haarnojaetal.,2018),self-driving art algorithms or require access to large amounts of com-\\n9\\ncars (Bansal, Krizhevsky, and Ogale, 2018), and control putational resources. At the same time, they may either be\\n1\\n: problems such as increasing the power efﬁciency of data (near-)deterministic or there may even be a known model\\nv centers (Lazic et al., 2018). Yet, the rapid progress in this of the environment (such as in Go or Chess). Similarly,\\nXi ﬁeld has been fueled by making agents play games such manylearningenvironmentsareinherentlysingleplayerby\\nas the iconic Atari console games (Bellemare et al., 2013; only modeling the interaction of an agent with a ﬁxed en-\\nr\\na Mnih et al., 2013), the ancient game of Go (Silver et al., vironmentortheyfocusonasingleaspectofreinforcement\\n2016), or professionally played video games like Dota 2 learningsuchascontinuouscontrolorsafety.Finally,learn-\\n(OpenAI, 2019) or Starcraft II (Vinyals et al., 2017). The ingenvironmentsmayhaverestrictivelicensesordependon\\nreason for this is simple: games provide challenging envi- closedsourcebinaries.\\nronments where new algorithms and ideas can be quickly ThishighlightstheneedforaRLenvironmentthatisnot\\ntestedinasafeandreproduciblemanner. only challenging from a learning standpoint and customiz-\\nWhile a variety of reinforcement learning environments able in terms of difﬁculty but also accessible for research\\nexist, they often come with a few drawbacks for research, bothintermsoflicensingandintermsofrequiredcomputa-\\ntionalresources.Moreover,suchanenvironmentshouldide-\\n∗Indicates equal authorship. Correspondence to Karol Kurach\\nally provide the tools to a variety of current reinforcement\\n(kkurach@google.com).\\nlearningresearchtopicssuchastheimpactofstochasticity,\\n†Student at Jagiellonian University, work done during intern-\\nself-play,multi-agentsetupsandmodel-basedreinforcement\\nshipatGoogleBrain.\\nCopyright(cid:13)c 2020,AssociationfortheAdvancementofArtiﬁcial learning, while also requiring smart decisions, tactics, and\\nIntelligence(www.aaai.org).Allrightsreserved. strategiesatmultiplelevelsofabstraction.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub('[^A-Za-z0-9]+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google Research Football A Novel Reinforcement Learning Environment KarolKurach AntonRaichuk cid 63 PiotrStan czyk cid 63 Micha Zaja c OlivierBachem LasseEspeholt CarlosRiquelme DamienVincent MarcinMichalski OlivierBousquet SylvainGelly GoogleResearch BrainTeam 0 2 0 Abstract 2 Recent progress in the eld of reinforcement learning has r p been accelerated by virtual learning environments such A as video games where novel algorithms and ideas can be quickly tested in a safe and reproducible manner We 4 introduce the Google Research Football Environment a 1 new reinforcement learning environment where agents are trained to play football in an advanced physics based 3D simulator The resulting environment is challenging easy G touseandcustomize anditisavailableunderapermissive L open source license In addition it provides support for multiplayer and multi agent experiments We propose three s full game scenarios of varying dif culty with the Football c Benchmarksandreportbaselineresultsforthreecommonly Figure 1 The Google Research Football Environment used reinforcement algorithms IMPALA PPO and Ape X github com google research football pro 2 DQN We also provide a diverse set of simpler scenarios v withtheFootballAcademyandshowcaseseveralpromising vides a novel reinforcement learning environment where 0 researchdirections agents are trained to play football in an advance physics 8 based3Dsimulation 1 Introduction 1 1 The goal of reinforcement learning RL is to train smart agents that can interact with their environment and solve which we discuss in detail in the next section For exam 7 complex tasks Sutton and Barto 2018 Real world appli ple they may either be too easy to solve for state of the 0 cationsincluderobotics Haarnojaetal 2018 self driving art algorithms or require access to large amounts of com 9 cars Bansal Krizhevsky and Ogale 2018 and control putational resources At the same time they may either be 1 problems such as increasing the power ef ciency of data near deterministic or there may even be a known model v centers Lazic et al 2018 Yet the rapid progress in this of the environment such as in Go or Chess Similarly Xi eld has been fueled by making agents play games such manylearningenvironmentsareinherentlysingleplayerby as the iconic Atari console games Bellemare et al 2013 only modeling the interaction of an agent with a xed en r a Mnih et al 2013 the ancient game of Go Silver et al vironmentortheyfocusonasingleaspectofreinforcement 2016 or professionally played video games like Dota 2 learningsuchascontinuouscontrolorsafety Finally learn OpenAI 2019 or Starcraft II Vinyals et al 2017 The ingenvironmentsmayhaverestrictivelicensesordependon reason for this is simple games provide challenging envi closedsourcebinaries ronments where new algorithms and ideas can be quickly ThishighlightstheneedforaRLenvironmentthatisnot testedinasafeandreproduciblemanner only challenging from a learning standpoint and customiz While a variety of reinforcement learning environments able in terms of dif culty but also accessible for research exist they often come with a few drawbacks for research bothintermsoflicensingandintermsofrequiredcomputa tionalresources Moreover suchanenvironmentshouldide Indicates equal authorship Correspondence to Karol Kurach ally provide the tools to a variety of current reinforcement kkurach google com learningresearchtopicssuchastheimpactofstochasticity Student at Jagiellonian University work done during intern self play multi agentsetupsandmodel basedreinforcement shipatGoogleBrain Copyright cid 13 c 2020 AssociationfortheAdvancementofArti cial learning while also requiring smart decisions tactics and Intelligence www aaai org Allrightsreserved strategiesatmultiplelevelsofabstraction '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/sebleier/554280\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(e for e in text.split() if e not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google Research Football A Novel Reinforcement Learning Environment KarolKurach AntonRaichuk cid 63 PiotrStan czyk cid 63 Micha Zaja c OlivierBachem LasseEspeholt CarlosRiquelme DamienVincent MarcinMichalski OlivierBousquet SylvainGelly GoogleResearch BrainTeam 0 2 0 Abstract 2 Recent progress eld reinforcement learning r p accelerated virtual learning environments A video games novel algorithms ideas quickly tested safe reproducible manner We 4 introduce Google Research Football Environment 1 new reinforcement learning environment agents trained play football advanced physics based 3D simulator The resulting environment challenging easy G touseandcustomize anditisavailableunderapermissive L open source license In addition provides support multiplayer multi agent experiments We propose three full game scenarios varying dif culty Football c Benchmarksandreportbaselineresultsforthreecommonly Figure 1 The Google Research Football Environment used reinforcement algorithms IMPALA PPO Ape X github com google research football pro 2 DQN We also provide diverse set simpler scenarios v withtheFootballAcademyandshowcaseseveralpromising vides novel reinforcement learning environment 0 researchdirections agents trained play football advance physics 8 based3Dsimulation 1 Introduction 1 1 The goal reinforcement learning RL train smart agents interact environment solve discuss detail next section For exam 7 complex tasks Sutton Barto 2018 Real world appli ple may either easy solve state 0 cationsincluderobotics Haarnojaetal 2018 self driving art algorithms require access large amounts com 9 cars Bansal Krizhevsky Ogale 2018 control putational resources At time may either 1 problems increasing power ef ciency data near deterministic may even known model v centers Lazic et al 2018 Yet rapid progress environment Go Chess Similarly Xi eld fueled making agents play games manylearningenvironmentsareinherentlysingleplayerby iconic Atari console games Bellemare et al 2013 modeling interaction agent xed en r Mnih et al 2013 ancient game Go Silver et al vironmentortheyfocusonasingleaspectofreinforcement 2016 professionally played video games like Dota 2 learningsuchascontinuouscontrolorsafety Finally learn OpenAI 2019 Starcraft II Vinyals et al 2017 The ingenvironmentsmayhaverestrictivelicensesordependon reason simple games provide challenging envi closedsourcebinaries ronments new algorithms ideas quickly ThishighlightstheneedforaRLenvironmentthatisnot testedinasafeandreproduciblemanner challenging learning standpoint customiz While variety reinforcement learning environments able terms dif culty also accessible research exist often come drawbacks research bothintermsoflicensingandintermsofrequiredcomputa tionalresources Moreover suchanenvironmentshouldide Indicates equal authorship Correspondence Karol Kurach ally provide tools variety current reinforcement kkurach google com learningresearchtopicssuchastheimpactofstochasticity Student Jagiellonian University work done intern self play multi agentsetupsandmodel basedreinforcement shipatGoogleBrain Copyright cid 13 c 2020 AssociationfortheAdvancementofArti cial learning also requiring smart decisions tactics Intelligence www aaai org Allrightsreserved strategiesatmultiplelevelsofabstraction'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google research football a novel reinforcement learning environment karolkurach antonraichuk cid 63 piotrstan czyk cid 63 micha zaja c olivierbachem lasseespeholt carlosriquelme damienvincent marcinmichalski olivierbousquet sylvaingelly googleresearch brainteam 0 2 0 abstract 2 recent progress eld reinforcement learning r p accelerated virtual learning environments a video games novel algorithms ideas quickly tested safe reproducible manner we 4 introduce google research football environment 1 new reinforcement learning environment agents trained play football advanced physics based 3d simulator the resulting environment challenging easy g touseandcustomize anditisavailableunderapermissive l open source license in addition provides support multiplayer multi agent experiments we propose three full game scenarios varying dif culty football c benchmarksandreportbaselineresultsforthreecommonly figure 1 the google research football environment used reinforcement algorithms impala ppo ape x github com google research football pro 2 dqn we also provide diverse set simpler scenarios v withthefootballacademyandshowcaseseveralpromising vides novel reinforcement learning environment 0 researchdirections agents trained play football advance physics 8 based3dsimulation 1 introduction 1 1 the goal reinforcement learning rl train smart agents interact environment solve discuss detail next section for exam 7 complex tasks sutton barto 2018 real world appli ple may either easy solve state 0 cationsincluderobotics haarnojaetal 2018 self driving art algorithms require access large amounts com 9 cars bansal krizhevsky ogale 2018 control putational resources at time may either 1 problems increasing power ef ciency data near deterministic may even known model v centers lazic et al 2018 yet rapid progress environment go chess similarly xi eld fueled making agents play games manylearningenvironmentsareinherentlysingleplayerby iconic atari console games bellemare et al 2013 modeling interaction agent xed en r mnih et al 2013 ancient game go silver et al vironmentortheyfocusonasingleaspectofreinforcement 2016 professionally played video games like dota 2 learningsuchascontinuouscontrolorsafety finally learn openai 2019 starcraft ii vinyals et al 2017 the ingenvironmentsmayhaverestrictivelicensesordependon reason simple games provide challenging envi closedsourcebinaries ronments new algorithms ideas quickly thishighlightstheneedforarlenvironmentthatisnot testedinasafeandreproduciblemanner challenging learning standpoint customiz while variety reinforcement learning environments able terms dif culty also accessible research exist often come drawbacks research bothintermsoflicensingandintermsofrequiredcomputa tionalresources moreover suchanenvironmentshouldide indicates equal authorship correspondence karol kurach ally provide tools variety current reinforcement kkurach google com learningresearchtopicssuchastheimpactofstochasticity student jagiellonian university work done intern self play multi agentsetupsandmodel basedreinforcement shipatgooglebrain copyright cid 13 c 2020 associationfortheadvancementofarti cial learning also requiring smart decisions tactics intelligence www aaai org allrightsreserved strategiesatmultiplelevelsofabstraction'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"preprocessed_text.txt\",\"wt\", encoding=\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.writelines(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
